{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Part 1: Environment Setup"
      ],
      "metadata": {
        "id": "0W37BNe08ymG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6djR88gS79gj"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ProductSalesAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2: Load Sales Data from CSV"
      ],
      "metadata": {
        "id": "fTakuUSo82Pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data=\"\"\"OrderID,Product,Category,Quantity,UnitPrice,Region\n",
        "1001,Mobile,Electronics,2,15000,North\n",
        "1002,Laptop,Electronics,1,55000,South\n",
        "1003,T-Shirt,Apparel,3,500,East\n",
        "1004,Jeans,Apparel,2,1200,North\n",
        "1005,TV,Electronics,1,40000,West\n",
        "1006,Shoes,Footwear,4,2000,South\n",
        "1007,Watch,Accessories,2,3000,East\n",
        "1008,Headphones,Electronics,3,2500,North\n",
        "\"\"\"\n",
        "with open('sales.csv','w') as f:\n",
        "  f.write(csv_data)\n",
        "\n",
        "df=spark.read.csv('sales.csv', header=True, inferSchema=True)\n",
        "df.show(5)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "wylrNlsv8t68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3: Business Questions\n",
        "1. Add a new column TotalPrice = Quantity Ã— UnitPrice"
      ],
      "metadata": {
        "id": "YsaW4umf-G01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"TotalPrice\", df[\"Quantity\"] * df[\"UnitPrice\"])\n",
        "df.show()"
      ],
      "metadata": {
        "id": "v3tY7mOC-LhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Total revenue generated across all regions."
      ],
      "metadata": {
        "id": "sGEAGEUp_Qjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum as _sum\n",
        "total_revenue = df.agg(_sum(\"TotalPrice\").alias(\"TotalRevenue\"))\n",
        "total_revenue.show()"
      ],
      "metadata": {
        "id": "kQi6QV53_R7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Category-wise revenue sorted in descending order."
      ],
      "metadata": {
        "id": "bklXoMr-A1XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_df=df.groupBy(\"Category\").sum(\"TotalPrice\").orderBy(\"sum(TotalPrice)\", ascending=False)\n",
        "category_df.show()\n"
      ],
      "metadata": {
        "id": "co1AoR3zA2wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Region with the highest number of orders"
      ],
      "metadata": {
        "id": "8Ln02-cHBUIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg_df=df.groupBy(\"Region\").count().orderBy(\"count\", ascending=False)\n",
        "reg_df.show(1)\n"
      ],
      "metadata": {
        "id": "lEzi3mZeBW6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Average Unit Price per Category"
      ],
      "metadata": {
        "id": "c1b5J7-7BmnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg=df.groupBy(\"Category\").avg(\"UnitPrice\")\n",
        "avg.show()\n"
      ],
      "metadata": {
        "id": "ZfMY7Ek_Br5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. All orders where TotalPrice is more than\n",
        "30,000"
      ],
      "metadata": {
        "id": "1bp3RMeHCIMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df[\"TotalPrice\"] > 30000).show()\n"
      ],
      "metadata": {
        "id": "tOhnFC49CKai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 4: Data Transformations\n",
        "1. Create a new column HighValueOrder which is \"Yes\" if TotalPrice > 20,000,\n",
        "else \"No\" ."
      ],
      "metadata": {
        "id": "IVAEUGPqDVAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "df = df.withColumn(\n",
        "    \"HighValueOrder\",\n",
        "    when(col(\"TotalPrice\") > 20000, \"Yes\").otherwise(\"No\")\n",
        ")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "iNoBebU4DZnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Filter and display all high-value orders in the North region."
      ],
      "metadata": {
        "id": "Mjk1xYeBDixs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high_value= df.filter((col(\"HighValueOrder\") == \"Yes\") & (col(\"Region\") == \"North\"))\n",
        "high_value.show()"
      ],
      "metadata": {
        "id": "-9P83hn_Dlq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Count how many high-value orders exist per region."
      ],
      "metadata": {
        "id": "yIou7RnMD0nC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(col(\"HighValueOrder\") == \"Yes\") \\\n",
        "  .groupBy(\"Region\") \\\n",
        "  .count() \\\n",
        "  .show()"
      ],
      "metadata": {
        "id": "zoadEk74Dzsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 5: Save Results\n"
      ],
      "metadata": {
        "id": "zVRs8F-jEEZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.coalesce(1).write.csv(\"high_value_orders\", header=True, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "15X0-IX-L_7F"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}